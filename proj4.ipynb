{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "#Karuna Gujar\n",
    "#CSCI 6350-001 Project #4\n",
    "#Due: 02/23/20\n",
    "\n",
    "#THis program uses the polarity and intensity of words to assign one of five ratings to \n",
    "#product reviews via a multinomial logistic regression classifier.\n",
    "######################################################\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import opinion_lexicon\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn import preprocessing\n",
    "\n",
    "file_name = 'reviews.txt'\n",
    "train_file = 'reviews-train.txt'\n",
    "test_file = 'reviews-test.txt'\n",
    "\n",
    "test_train_div = 0.2\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#lemmatizing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "### THis function reads the input file and  divides it into training and testing files for processing. \n",
    "### The test-train ratio is 20:80 \n",
    "def read_input_file(filename):\n",
    "    with open(filename, 'r', encoding='utf8') as file:\n",
    "        lines = []\n",
    "        category = []\n",
    "        for line in file:\n",
    "\n",
    "            if line != '\\n':\n",
    "                vals = line.split()\n",
    "                category.append(vals[-1])\n",
    "                line = ''\n",
    "                for i in range(len(vals)-1):\n",
    "                    line = line + ' ' + vals [i]\n",
    "                lines.append(line)\n",
    "    df_review_file = pd.DataFrame({'label':category, 'review':lines})\n",
    "    \n",
    "    \n",
    "    Y = df_review_file.loc[:,'label'].values\n",
    "    X = df_review_file.loc[:,'review'].values\n",
    "\n",
    "    train_x, test_x, train_y, test_y = train_test_split(X,Y , test_size=test_train_div, random_state=0, stratify=Y)\n",
    "    \n",
    "    \n",
    "    before_upsample = pd.DataFrame({'label':train_y, 'review':train_x})\n",
    "    test_df = pd.DataFrame({'label':test_y, 'review':test_x})\n",
    "       \n",
    "    test_df.to_csv(test_file,sep=\"|\")\n",
    "    before_upsample.to_csv(train_file,sep=\"|\")\n",
    "    \n",
    "    return before_upsample, test_df\n",
    "\n",
    "\n",
    "train_df, test_df = read_input_file(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### THis function tokenizes and cleans the tokens by removing punctuation, \n",
    "### lemmatizig and converting into lower case.\n",
    "def preprocess_and_tokenize( comment):\n",
    "    \n",
    "    tokens = word_tokenize(comment)\n",
    "    # remove puntuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    token_list = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        token = token.translate(table)\n",
    "\n",
    "        #remove punctuations from non-aplhabetic words\n",
    "        if(token.isalpha()):\n",
    "            token = lemmatizer.lemmatize(token)\n",
    "            token_list.append(token.lower())\n",
    "\n",
    "    return token_list\n",
    "\n",
    "### adding respective tokens to train and test dataframes\n",
    "train_df[\"tokens\"]=train_df.review.apply(lambda x:preprocess_and_tokenize(x))\n",
    "test_df[\"tokens\"]=test_df.review.apply(lambda x:preprocess_and_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using frequency distribution to find 2000 most commonly seen unigrams in the reviews\n",
    "all_train_words = [ x for item in train_df[\"tokens\"].values.tolist() for x in item ]\n",
    "all_train_words = nltk.FreqDist(all_train_words)\n",
    "all_train_words = all_train_words.most_common(2000)\n",
    "all_train_words = [x[0] for x in all_train_words]\n",
    "\n",
    "### one-hot encoding for the unigrams in the reviews depending on the most common 2000 unigrams\n",
    "def get_ohe( x, all_words):\n",
    "    return [1 if word in x else 0 for word in all_words]\n",
    "\n",
    "### Adding one-hot encoded unigrams to the test and train dataframes\n",
    "train_df[\"unigrams_vec\"] = train_df.tokens.apply(lambda x:get_ohe(x,all_train_words))\n",
    "test_df[\"unigrams_vec\"] = test_df.tokens.apply(lambda x:get_ohe(x,all_train_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### extracting bigrams\n",
    "def get_bigrams( tokens):\n",
    "    bigrm = nltk.bigrams(tokens)\n",
    "    return [*map(' '.join, bigrm)]\n",
    "\n",
    "\n",
    "train_df[\"bi_tokens\"]=train_df.tokens.apply(lambda x:get_bigrams(x))\n",
    "test_df[\"bi_tokens\"]=test_df.tokens.apply(lambda x:get_bigrams(x))\n",
    "\n",
    "### Using frequency distribution to find 1000 most commonly seen bigrams in the reviews\n",
    "all_train_bigrams = [ x for item in train_df[\"bi_tokens\"].values.tolist() for x in item ]\n",
    "all_train_bigrams = nltk.FreqDist(all_train_bigrams)\n",
    "all_train_bigrams = all_train_bigrams.most_common(1000)\n",
    "all_train_bigrams = [x[0] for x in all_train_bigrams]\n",
    "\n",
    "### one-hot encoding for the bigrams in the reviews depending on the most common 1000 bigrams\n",
    "train_df[\"bigram_vec\"] = train_df.bi_tokens.apply(lambda x:get_ohe(x,all_train_bigrams))\n",
    "test_df[\"bigram_vec\"] = test_df.bi_tokens.apply(lambda x:get_ohe(x,all_train_bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### combining unigrams and bigrams vectors\n",
    "train_df[\"combined\"] = train_df.apply(lambda x:x.unigrams_vec + x.bigram_vec,axis=1)\n",
    "test_df[\"combined\"] = test_df.apply(lambda x:x.unigrams_vec + x.bigram_vec,axis=1)\n",
    "\n",
    "Y = train_df.label.values\n",
    "X = train_df.combined.values.tolist()\n",
    "\n",
    "ytest = test_df.label.values\n",
    "xtest = test_df.combined.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multinomial Accuracy:  0.576\n",
      "multinomial Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.27      0.40      0.32        10\n",
      "           2       0.14      0.27      0.18        15\n",
      "           3       0.13      0.26      0.18        34\n",
      "           4       0.35      0.36      0.36        99\n",
      "           5       0.82      0.69      0.75       342\n",
      "\n",
      "    accuracy                           0.58       500\n",
      "   macro avg       0.34      0.40      0.36       500\n",
      "weighted avg       0.65      0.58      0.61       500\n",
      "\n",
      "multinomial Classification Report: \n",
      " [[  4   1   4   0   1]\n",
      " [  1   4   5   3   2]\n",
      " [  1   5   9  10   9]\n",
      " [  1   7  15  36  40]\n",
      " [  8  12  34  53 235]]\n",
      "ovr Accuracy:  0.606\n",
      "ovr Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.29      0.20      0.24        10\n",
      "           2       0.12      0.13      0.12        15\n",
      "           3       0.18      0.29      0.22        34\n",
      "           4       0.36      0.43      0.39        99\n",
      "           5       0.82      0.72      0.77       342\n",
      "\n",
      "    accuracy                           0.61       500\n",
      "   macro avg       0.35      0.36      0.35       500\n",
      "weighted avg       0.65      0.61      0.63       500\n",
      "\n",
      "ovr Classification Report: \n",
      " [[  2   1   4   2   1]\n",
      " [  0   2   6   5   2]\n",
      " [  0   3  10  11  10]\n",
      " [  0   5  10  43  41]\n",
      " [  5   6  26  59 246]]\n"
     ]
    }
   ],
   "source": [
    "### \"Penalty\" parameter is used for regularization. If \"penalty\" is not mentioned explicitly, \n",
    "### no regularization is applied. \"l2\" is a type of regularization. \n",
    "### The ‘newton-cg’, ‘sag’ and ‘lbfgs’ solvers support only l2 penalties\n",
    "\n",
    "### The \"C\" parameter in the LogisticRegression function is used to decide the strength of regularization. \n",
    "### It is inverse of regularization strength and must be a positive float. \n",
    "### Smaller values of C specify stronger regularization.\n",
    "\n",
    "### \"Class weight\" pamater is used to decide how much weight to be given to each class.\n",
    "### The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to \n",
    "### class frequencies in the input data. Since the reviews data we had, was very unbiased, \n",
    "### I decided to use this parameter.\n",
    "\n",
    "for mclass in ('multinomial', 'ovr'):\n",
    "    lr = LogisticRegression(solver='lbfgs', max_iter=100, C=0.01,\n",
    "                            random_state=0, multi_class=mclass, \\\n",
    "                            dual=False,penalty=\"l2\",class_weight=\"balanced\").fit(X, Y)\n",
    "    yhat = lr.predict(xtest)\n",
    "\n",
    "    print(mclass,'Accuracy: ',accuracy_score(ytest, yhat))\n",
    "    print(mclass,'Classification Report: \\n',classification_report(ytest, yhat))\n",
    "    print(mclass,'Classification Report: \\n',confusion_matrix(ytest, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tSelection of features: I am using 2000 most commonly used unigrams and 1000 most commonly used bigrams as the features. \n",
    "I started with \n",
    "    a.\ttotal length of the review\n",
    "    b.\tpresence of number of positive words and negative words in the reviews\n",
    "    c.\tc. most common top 2000 words\n",
    "and observed that the length of the review was not very helpful. Removing that feature did not cause any loss in the accuracy.\n",
    "Another observation was that positive and negative words did not help because the reference in which these (positive or negative) words were used was equally important, merely getting the counts wasn’t sufficient. Funny thing, the review with rating 5 sometimes had a greater number of negative words than positive. Hence this feature wasn’t helpful. \n",
    "So I decided to stick to only the commonly used words feature. To make it more precise I analyzed using both unigrams and bigrams. Another interesting thing I noticed was, not removing the stop words actually bumped up the accuracy. \n",
    "Also, I observed that the data given was biased or unsymmetrical, that is, the number of data points for rating 5 outsized the number of data points for rating 1 or 2 or 3. So I looked into data imbalance and found that the solution to fix it is upsclaing the inferiror classes. I worked on it for a while and realized that did not improve the performance much if i considered threshhold as 10% of supperior class (required threshold to be 60%). Later found about the \"class_weight\" parameter in logistic regression fumction which is used to assign weights to the classes depending on the balance. \"Class weight\" pamater is used to decide how much weight to be given to each class. The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data. This dropped the accuracy but gave better precision, recall and F1 values.\n",
    "\n",
    "2.\tI got better accuracy and precision with ovr than multinomial. However, recall and F1 score is better with multinomial. \n",
    "\n",
    "3.\tAccuracy is ratio of correctly predicted observation to the total observations. Accuracy is a great measure  to gauge the performance but only with symmetric datasets where values of false positive and false negatives are almost same. I got 58% accuracy with Multinomial classification and 60% with ovr. However, the reviews data given was highly unsymmetrical. Thus looking at other metrics is important to gauge the performance. Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. Precision for class 5 is better than others, in both multinomial and ovr. The reason being, there were lot of data samples for class 5 resulting in the model trained better for review 5 than others.\n",
    "Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes. Can also be termed as sensitivity. F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. F1 is usually more useful than accuracy, especially in case of uneven class distribution. I got F1 0.75 for multinomial and 0.77 for ovr for class 5.\n",
    "A confusion matrix shows the combination of the actual and predicted classes. Each row of the matrix represents the instances in a predicted class, while each column represents the instances in an actual class. It is a good measure of whether models can account for the overlap in class properties and understand which classes are most easily confused.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REGULARIZATION\n",
    "\n",
    "SkLearn uses parameter “Penalty” in the logisticRegression function to indicate regularization. \"Penalty\" parameter is used for regularization. If \"penalty\" is not mentioned explicitly, no regularization is applied. \"l2\" is a type of regularization. \n",
    "The ‘newton-cg’, ‘sag’ and ‘lbfgs’ solvers support only l2 penalties. The \"C\" parameter in the LogisticRegression function is used to decide the strength of regularization. It is inverse of regularization strength and must be a positive float. \n",
    "Smaller values of C specify stronger regularization. So I selected C=0.01.\n",
    "\n",
    "Regularization can be used to avoid overfitting. Regularization can be used to train models that generalize better on unseen data, by preventing the algorithm from overfitting the training dataset. \n",
    "\n",
    "Accuracy increased from 58% to 59% in multinomial without regularizing, where as it dropped in ovr from 60% to 59%. More importantly, precision and F1 score dropped for both ovr and multinomial without regularizing.\n",
    "\n",
    "Following is the output without regularization: \n",
    "\n",
    "\n",
    "multinomial Accuracy:  0.596\n",
    "multinomial Classification Report: \n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           1       0.50      0.20      0.29        10\n",
    "           2       0.08      0.07      0.07        15\n",
    "           3       0.17      0.21      0.18        34\n",
    "           4       0.30      0.32      0.31        99\n",
    "           5       0.77      0.75      0.76       342\n",
    "\n",
    "    accuracy                           0.60       500\n",
    "   macro avg       0.36      0.31      0.32       500\n",
    "weighted avg       0.61      0.60      0.60       500\n",
    "\n",
    "multinomial Classification Report: \n",
    " [[  2   1   4   1   2]\n",
    " [  0   1   5   4   5]\n",
    " [  0   3   7  12  12]\n",
    " [  0   1   7  32  59]\n",
    " [  2   7  19  58 256]]\n",
    "ovr Accuracy:  0.594\n",
    "ovr Classification Report: \n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           1       0.50      0.10      0.17        10\n",
    "           2       0.12      0.07      0.09        15\n",
    "           3       0.14      0.15      0.14        34\n",
    "           4       0.29      0.32      0.30        99\n",
    "           5       0.76      0.75      0.76       342\n",
    "\n",
    "    accuracy                           0.59       500\n",
    "   macro avg       0.36      0.28      0.29       500\n",
    "weighted avg       0.60      0.59      0.59       500\n",
    "\n",
    "ovr Classification Report: \n",
    " [[  1   1   3   3   2]\n",
    " [  0   1   5   3   6]\n",
    " [  0   3   5  11  15]\n",
    " [  0   0   7  32  60]\n",
    " [  1   3  17  63 258]]\n",
    "\n",
    "\n",
    "\n",
    "FOllowing is the code that is used for regularizing:\n",
    "\n",
    "lr = LogisticRegression(solver='lbfgs', max_iter=100, C=0.01,\n",
    "                            random_state=0, multi_class=mclass, \\\n",
    "                            dual=False,penalty=\"l2\",class_weight=\"balanced\").fit(X, Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
